# -*- coding: utf-8 -*-
"""Copy of Prosess_mining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oGLGhryGzfnWp7L59cwsSQwIThAjNU3U
"""

!pip -q install pm4py pandas numpy matplotlib graphviz

# (ΠΡΟΑΙΡΕΤΙΚΟ αλλά βοηθάει σε κάποια Colab runtimes)
!apt-get -qq update
!apt-get -qq install -y graphviz
print("✅ Installed")

from google.colab import files
uploaded = files.upload()  # ανέβασε new_dump.json

import json
import pandas as pd
import numpy as np

INPUT = "new_dump.json"

records = []
with open(INPUT, "r", encoding="utf-8") as f:
    for line in f:
        line = line.strip()
        if not line:
            continue
        records.append(json.loads(line))

df_raw = pd.DataFrame(records)
print("✅ Raw events:", len(df_raw))
df_raw.head(5)

import pm4py

KEPT_MSGS = {"Robot started task", "Robot ended task", "Chair assembled"}

def build_gregor_event_log_v1(df_raw: pd.DataFrame) -> pd.DataFrame:
    df = df_raw.copy()

    # keep only V1 messages
    df = df[df["msg"].isin(KEPT_MSGS)].copy()

    # timestamp
    df["time:timestamp"] = pd.to_datetime(df["time"], errors="coerce", utc=True)

    # case id: prefer Chair; else parse "Number of chair:"
    chair_from_number = pd.to_numeric(df.get("Number of chair:", pd.Series([np.nan]*len(df))), errors="coerce")
    df["case:concept:name"] = pd.to_numeric(df.get("Chair", pd.Series([np.nan]*len(df))), errors="coerce")
    df["case:concept:name"] = df["case:concept:name"].fillna(chair_from_number)

    # activity mapping (όπως το report v1)
    df["concept:name"] = np.where(
        df["msg"].eq("Robot started task"),
        df["Task"].astype(str) + ":start",
        np.where(
            df["msg"].eq("Robot ended task"),
            df["Task"].astype(str) + ":end",
            "Chair:assembled"
        )
    )

    # keep attributes we want to carry
    keep_cols = [
        "case:concept:name", "concept:name", "time:timestamp",
        "Robot", "Task", "Caller", "Workbench", "Fixture",
        "Storage", "Item", "From", "Component", "Conveyor Belt",
        "level", "msg"
    ]
    keep_cols = [c for c in keep_cols if c in df.columns]
    df = df[keep_cols].copy()

    # drop rows missing core columns
    before = len(df)
    df = df.dropna(subset=["case:concept:name", "time:timestamp", "concept:name"]).copy()
    dropped = before - len(df)

    # format for pm4py
    df = pm4py.format_dataframe(
        df,
        case_id="case:concept:name",
        activity_key="concept:name",
        timestamp_key="time:timestamp"
    )
    return df, dropped

event_log_v1, dropped = build_gregor_event_log_v1(df_raw)

print("✅ v1 events:", len(event_log_v1))
print("✅ v1 traces:", event_log_v1["case:concept:name"].nunique())
print("✅ v1 activities:", event_log_v1["concept:name"].nunique())
print("Dropped (missing case or ts):", dropped)

event_log_v1.head(15)

from google.colab import files

event_log_v1.to_csv("gregor_event_log_v1.csv", index=False)

log_obj = pm4py.convert_to_event_log(event_log_v1)
pm4py.write_xes(log_obj, "gregor_event_log_v1.xes")

print("✅ Saved: gregor_event_log_v1.csv, gregor_event_log_v1.xes")

import pm4py

log_obj = pm4py.convert_to_event_log(event_log_v1)

models = {}

# Alpha Miner
net_a, im_a, fm_a = pm4py.discover_petri_net_alpha(log_obj)
models["alpha"] = (net_a, im_a, fm_a)

# Inductive Miner
net_i, im_i, fm_i = pm4py.discover_petri_net_inductive(log_obj)
models["inductive"] = (net_i, im_i, fm_i)

# Heuristics Miner
net_h, im_h, fm_h = pm4py.discover_petri_net_heuristics(log_obj)
models["heuristics"] = (net_h, im_h, fm_h)

# Save figures
for name, (net, im, fm) in models.items():
    gviz = pm4py.visualization.petri_net.visualizer.apply(net, im, fm)
    pm4py.visualization.petri_net.visualizer.save(gviz, f"{name}_petri.png")

print("✅ Saved: alpha_petri.png, inductive_petri.png, heuristics_petri.png")

from google.colab import files

for fn in ["alpha_petri.png", "inductive_petri.png", "heuristics_petri.png"]:
    files.download(fn)

import pandas as pd
import pm4py

results = []

for name, (net, im, fm) in models.items():
    row = {"model": name}

    # Token-based replay fitness
    tbr = pm4py.fitness_token_based_replay(log_obj, net, im, fm)
    row["tbr_log_fitness"] = float(tbr.get("log_fitness", np.nan))
    row["tbr_avg_trace_fitness"] = float(tbr.get("average_trace_fitness", np.nan))
    row["tbr_perc_fit_traces"] = float(tbr.get("percentage_of_fitting_traces", np.nan))

    # Alignments-based fitness + precision (μπορεί να αποτύχει σε κάποια nets, π.χ. alpha)
    try:
        af = pm4py.fitness_alignments(log_obj, net, im, fm)
        row["align_log_fitness"] = float(af.get("log_fitness", np.nan))
        row["align_avg_trace_fitness"] = float(af.get("average_trace_fitness", np.nan))
        row["align_perc_fit_traces"] = float(af.get("percentage_of_fitting_traces", np.nan))

        prec = pm4py.precision_alignments(log_obj, net, im, fm)
        row["align_precision"] = float(prec)
        row["notes"] = None
    except Exception as e:
        row["align_log_fitness"] = np.nan
        row["align_avg_trace_fitness"] = np.nan
        row["align_perc_fit_traces"] = np.nan
        row["align_precision"] = np.nan
        row["notes"] = str(e)

    results.append(row)

df_conf = pd.DataFrame(results)
df_conf.to_csv("gregor_v1_conformance_results.csv", index=False)
print("✅ Saved: gregor_v1_conformance_results.csv")
df_conf

case_times = (
    event_log_v1
    .groupby("case:concept:name")["time:timestamp"]
    .agg(case_start="min", case_end="max")
    .reset_index()
)

case_times["throughput_sec"] = (case_times["case_end"] - case_times["case_start"]).dt.total_seconds()

case_times.to_csv("gregor_v1_case_throughput.csv", index=False)
print("✅ Saved: gregor_v1_case_throughput.csv")
case_times

import re

df = event_log_v1.copy()
df = df.sort_values(["case:concept:name", "time:timestamp"])

# isolate task start/end events
mask = df["concept:name"].str.contains(r"^AssemblyTask\d+:(start|end)$", regex=True, na=False)
df_tasks = df[mask].copy()

df_tasks["task"] = df_tasks["concept:name"].str.replace(r":(start|end)$", "", regex=True)
df_tasks["lifecycle"] = df_tasks["concept:name"].str.extract(r":(start|end)$", expand=False)

pairs = []

for case_id, g in df_tasks.groupby("case:concept:name"):
    # for each task, pair in time order
    for task, gt in g.groupby("task"):
        gt = gt.sort_values("time:timestamp")
        starts = gt[gt["lifecycle"] == "start"]
        ends = gt[gt["lifecycle"] == "end"]

        # naive pairing by order (works well if log is clean)
        n = min(len(starts), len(ends))
        for i in range(n):
            s = starts.iloc[i]
            e = ends.iloc[i]
            dur = (e["time:timestamp"] - s["time:timestamp"]).total_seconds()
            pairs.append({
                "case:concept:name": case_id,
                "task": task,
                "start_ts": s["time:timestamp"],
                "end_ts": e["time:timestamp"],
                "duration_sec": dur,
                "Robot_start": s.get("Robot", None),
                "Robot_end": e.get("Robot", None),
            })

task_durations = pd.DataFrame(pairs)
task_durations.to_csv("gregor_v1_task_durations.csv", index=False)
print("✅ Saved: gregor_v1_task_durations.csv")
task_durations.head(10)

import re

df = event_log_v1.copy()
df = df.sort_values(["case:concept:name", "time:timestamp"])

# isolate task start/end events
mask = df["concept:name"].str.contains(r"^AssemblyTask\d+:(start|end)$", regex=True, na=False)
df_tasks = df[mask].copy()

df_tasks["task"] = df_tasks["concept:name"].str.replace(r":(start|end)$", "", regex=True)
df_tasks["lifecycle"] = df_tasks["concept:name"].str.extract(r":(start|end)$", expand=False)

pairs = []

for case_id, g in df_tasks.groupby("case:concept:name"):
    # for each task, pair in time order
    for task, gt in g.groupby("task"):
        gt = gt.sort_values("time:timestamp")
        starts = gt[gt["lifecycle"] == "start"]
        ends = gt[gt["lifecycle"] == "end"]

        # naive pairing by order (works well if log is clean)
        n = min(len(starts), len(ends))
        for i in range(n):
            s = starts.iloc[i]
            e = ends.iloc[i]
            dur = (e["time:timestamp"] - s["time:timestamp"]).total_seconds()
            pairs.append({
                "case:concept:name": case_id,
                "task": task,
                "start_ts": s["time:timestamp"],
                "end_ts": e["time:timestamp"],
                "duration_sec": dur,
                "Robot_start": s.get("Robot", None),
                "Robot_end": e.get("Robot", None),
            })

task_durations = pd.DataFrame(pairs)
task_durations.to_csv("gregor_v1_task_durations.csv", index=False)
print("✅ Saved: gregor_v1_task_durations.csv")
task_durations.head(10)

task_stats = (
    task_durations
    .groupby("task")["duration_sec"]
    .agg(["count", "mean", "median", "std", "min", "max"])
    .reset_index()
)

task_stats.to_csv("gregor_v1_task_duration_stats.csv", index=False)
print("✅ Saved: gregor_v1_task_duration_stats.csv")
task_stats

def compute_fixture_wait_times(df_raw: pd.DataFrame) -> pd.DataFrame:
    df = df_raw.copy()
    df["ts"] = pd.to_datetime(df["time"], errors="coerce", utc=True)
    df = df.dropna(subset=["ts", "msg"]).sort_values("ts").copy()

    req = df[df["msg"] == "Fixture requested"].copy()
    grn = df[df["msg"] == "Fixture request granted"].copy()

    # Κλειδί που “δένει” request με grant.
    # (Το Chair μπορεί να μην ταιριάζει πάντα, άρα ΔΕΝ το βάζουμε στο key)
    key_cols = ["Task", "Caller", "Workbench", "Fixture"]
    for c in key_cols:
        if c not in df.columns:
            df[c] = np.nan

    # queue ανά key (πολλαπλά outstanding requests)
    queues = {}
    rows = []

    def make_key(r):
        return tuple(r.get(c, None) for c in key_cols)

    for _, r in req.iterrows():
        k = make_key(r)
        queues.setdefault(k, []).append(r)

    # κάνουμε δεύτερο πέρασμα με grants, σε χρονολογική σειρά, και “τραβάμε” το παλαιότερο request
    queues = {}
    for _, r in req.sort_values("ts").iterrows():
        k = make_key(r)
        queues.setdefault(k, []).append(r)

    for _, g in grn.sort_values("ts").iterrows():
        k = make_key(g)
        if k not in queues or len(queues[k]) == 0:
            continue
        r = queues[k].pop(0)
        rows.append({
            "Task": g.get("Task", None),
            "Caller": g.get("Caller", None),
            "Workbench": g.get("Workbench", None),
            "Fixture": g.get("Fixture", None),
            "request_ts": r["ts"],
            "grant_ts": g["ts"],
            "wait_sec": (g["ts"] - r["ts"]).total_seconds(),
            "chair_req": r.get("Chair", np.nan),
            "chair_grant": g.get("Chair", np.nan),
        })

    return pd.DataFrame(rows)

fixture_wait_times = compute_fixture_wait_times(df_raw)
fixture_wait_times.to_csv("gregor_v2_fixture_wait_times.csv", index=False)

fixture_wait_stats = (
    fixture_wait_times
    .groupby(["Workbench", "Fixture", "Task", "Caller"])["wait_sec"]
    .agg(["count", "mean", "median", "std", "min", "max"])
    .reset_index()
    .sort_values(["median", "mean"], ascending=False)
)

fixture_wait_stats.to_csv("gregor_v2_fixture_wait_stats.csv", index=False)

print("✅ Saved: gregor_v2_fixture_wait_times.csv, gregor_v2_fixture_wait_stats.csv")
fixture_wait_stats.head(20)

!zip -q gregor_outputs.zip \
  gregor_event_log_v1.csv gregor_event_log_v1.xes \
  alpha_petri.png inductive_petri.png heuristics_petri.png \
  gregor_v1_conformance_results.csv \
  gregor_v1_case_throughput.csv \
  gregor_v1_task_durations.csv gregor_v1_task_duration_stats.csv \
  gregor_v2_fixture_wait_times.csv gregor_v2_fixture_wait_stats.csv

from google.colab import files
files.download("gregor_outputs.zip")

import pandas as pd
import numpy as np

df = event_log_v1.copy()
df["time:timestamp"] = pd.to_datetime(df["time:timestamp"], utc=True)

# 1) Ποια cases είναι "complete";
complete_cases = set(
    df.loc[df["concept:name"] == "Chair:assembled", "case:concept:name"].unique()
)

# 2) Throughput per case
case_times = (
    df.groupby("case:concept:name")["time:timestamp"]
      .agg(case_start="min", case_end="max")
      .reset_index()
)

case_times["throughput_sec"] = (case_times["case_end"] - case_times["case_start"]).dt.total_seconds()
case_times["is_complete"] = case_times["case:concept:name"].isin(complete_cases)

# 3) KPI: production rate (chairs/hour) για complete cases
case_times["chairs_per_hour"] = np.where(
    case_times["throughput_sec"] > 0,
    3600.0 / case_times["throughput_sec"],
    np.nan
)

complete = case_times[case_times["is_complete"]].copy()

print("Complete cases:", len(complete), " / Total cases:", len(case_times))
print("Throughput (sec) - complete cases:\n", complete["throughput_sec"].describe())
print("Production rate (chairs/hour) - complete cases:\n", complete["chairs_per_hour"].describe())

# 4) Save για “ευρήματα”
case_times.to_csv("gregor_kpi_case_times.csv", index=False)
print("✅ Saved: gregor_kpi_case_times.csv")

from google.colab import files
files.download("gregor_kpi_case_times.csv")

from google.colab import files
uploaded = files.upload()  # ανέβασε gregor_outputs.zip (ή τα 3 csv)

import zipfile
import pandas as pd

# Βρες το πρώτο .zip που ανέβηκε
zip_files = [fn for fn in uploaded.keys() if fn.lower().endswith(".zip")]
if not zip_files:
    raise FileNotFoundError("❌ Δεν ανέβασες .zip. Ανέβασε gregor_outputs.zip ή ανέβασε τα CSV ξεχωριστά.")
ZIP_PATH = zip_files[0]
print("✅ Using ZIP:", ZIP_PATH)

def read_csv_anywhere_in_zip(zipf, filename):
    candidates = [
        n for n in zipf.namelist()
        if (n.endswith("/" + filename) or n.endswith(filename))
        and (not n.startswith("__MACOSX"))
        and (not n.endswith("/"))
    ]
    if not candidates:
        # για debug: δείξε τι υπάρχει
        available = [n for n in zipf.namelist() if not n.endswith("/")]
        raise KeyError(f"❌ Δεν βρέθηκε '{filename}' μέσα στο zip.\n"
                       f"Περιεχόμενα zip (δείγμα): {available[:30]}")
    return pd.read_csv(zipf.open(candidates[0]))

with zipfile.ZipFile(ZIP_PATH) as z:
    case_thr = read_csv_anywhere_in_zip(z, "gregor_v1_case_throughput.csv")
    task_dur = read_csv_anywhere_in_zip(z, "gregor_v1_task_durations.csv")
    fw_times = read_csv_anywhere_in_zip(z, "gregor_v2_fixture_wait_times.csv")

print("✅ Loaded:",
      "case_thr:", len(case_thr),
      "| task_dur:", len(task_dur),
      "| fw_times:", len(fw_times))

case_thr.head(3)

import numpy as np
import pandas as pd

# Clean task durations
task_dur["duration_sec"] = pd.to_numeric(task_dur["duration_sec"], errors="coerce")
task_dur_clean = task_dur.dropna(subset=["duration_sec"]).copy()
task_dur_clean = task_dur_clean[task_dur_clean["duration_sec"] >= 0]

# Clean fixture waits
fw_times["wait_sec"] = pd.to_numeric(fw_times["wait_sec"], errors="coerce")
fw_clean = fw_times.dropna(subset=["wait_sec"]).copy()
fw_clean = fw_clean[fw_clean["wait_sec"] >= 0]

# A) Task bottleneck
task_bottleneck = (
    task_dur_clean
    .groupby(["task"], as_index=False)
    .agg(
        count=("duration_sec","count"),
        mean=("duration_sec","mean"),
        median=("duration_sec","median"),
        p95=("duration_sec", lambda x: np.quantile(x, 0.95))
    )
    .sort_values(["median","p95"], ascending=False)
)

# B) Fixture bottleneck
fixture_bottleneck = (
    fw_clean
    .groupby(["Task","Workbench","Fixture"], as_index=False)
    .agg(
        count=("wait_sec","count"),
        mean=("wait_sec","mean"),
        median=("wait_sec","median"),
        p95=("wait_sec", lambda x: np.quantile(x, 0.95))
    )
    .sort_values(["median","p95"], ascending=False)
)

# C) Join to KPI (case throughput)
# chair id σε fw_times είναι συνήθως chair_req, αλλά αν λείπει, fallback στο Chair
chair_col = "chair_req" if "chair_req" in fw_clean.columns else ("Chair" if "Chair" in fw_clean.columns else None)

if chair_col is None:
    print("⚠️ Δεν βρέθηκε chair id στο fixture_wait_times (chair_req/Chair). Το KPI join θα έχει NaN fixture totals.")
    fw_case = pd.DataFrame({"case_id": [], "total_fixture_wait_sec": []})
else:
    fw_case = (
        fw_clean.assign(case_id=pd.to_numeric(fw_clean[chair_col], errors="coerce"))
        .dropna(subset=["case_id"])
        .groupby("case_id", as_index=False)["wait_sec"].sum()
        .rename(columns={"wait_sec":"total_fixture_wait_sec"})
    )

task_case = (
    task_dur_clean.assign(case_id=pd.to_numeric(task_dur_clean["case:concept:name"], errors="coerce"))
    .dropna(subset=["case_id"])
    .groupby("case_id", as_index=False)["duration_sec"].sum()
    .rename(columns={"duration_sec":"total_task_processing_sec"})
)

case_thr2 = case_thr.copy()
case_thr2["case_id"] = pd.to_numeric(case_thr2["case:concept:name"], errors="coerce")

kpi_join = (
    case_thr2
    .merge(task_case, on="case_id", how="left")
    .merge(fw_case, on="case_id", how="left")
)

# Save
task_bottleneck.to_csv("bottleneck_tasks.csv", index=False)
fixture_bottleneck.to_csv("bottleneck_fixtures.csv", index=False)
kpi_join.to_csv("bottleneck_kpi_join.csv", index=False)

print("✅ Saved: bottleneck_tasks.csv, bottleneck_fixtures.csv, bottleneck_kpi_join.csv")

display(task_bottleneck.head(10))
display(fixture_bottleneck.head(10))
display(kpi_join.head(10))

from google.colab import files
files.download("bottleneck_tasks.csv")
files.download("bottleneck_fixtures.csv")
files.download("bottleneck_kpi_join.csv")

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("bottleneck_fixtures.csv")

# Πάρε τα top-10 (με βάση median)
top = df.sort_values("median", ascending=False).head(10).copy()

fig, ax = plt.subplots(figsize=(12, 3.5))
ax.axis("off")

table = ax.table(
    cellText=top.values,
    colLabels=top.columns,
    cellLoc="center",
    loc="center"
)

table.auto_set_font_size(False)
table.set_fontsize(9)
table.scale(1, 1.4)

plt.tight_layout()
plt.savefig("bottleneck_fixtures_top10.png", dpi=200)
plt.show()

from google.colab import files
files.download("bottleneck_fixtures_top10.png")



